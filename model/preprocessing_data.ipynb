{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec7aa91",
   "metadata": {},
   "source": [
    "# Препроцессинг данных\n",
    "В процессе предобработки данных производится их подготовка к анализу, в результате которой они приводятся в соответствие с требованиями, определяемыми спецификой решаемой задачи.\n",
    "Предобработка является важнейшим этапом при создании модели, и если она не будет выполнена, то дальнейший анализ в большинстве случаев невозможен из-за того, что аналитические алгоритмы просто не смогут работать или результаты их работы будут некорректными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4984a6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-06-03T20:23:29.371236Z",
     "iopub.status.busy": "2022-06-03T20:23:29.370566Z",
     "iopub.status.idle": "2022-06-03T20:23:43.820920Z",
     "shell.execute_reply": "2022-06-03T20:23:43.819966Z"
    },
    "papermill": {
     "duration": 14.458206,
     "end_time": "2022-06-03T20:23:43.823531",
     "exception": false,
     "start_time": "2022-06-03T20:23:29.365325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\r\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\r\n",
      "Collecting dawg-python>=0.7.1\r\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\r\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\r\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#  Выгрузим необходимые библиотеки и подгрузим модули для них.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install pymorphy2\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e4d2c",
   "metadata": {},
   "source": [
    "## Токенизация и лемматизация\n",
    "Токенизация — самый первый шаг при обработке текста, результатом которого является набор (список) так называемых токенов (подстрок). Они могут быть предложениями, словами или даже отдельными символами. \n",
    "Иными словами, токенизация - это выделение из описания товаров основных, выделяющихся слов или фраз.\n",
    "\n",
    "Лемматизация – объединение слов с одним и тем же корнем или леммой, но с разными склонениями или производными значения для дальнейшего анализа как элемента. Например, лемматизировать слова «кошки», «кошек» и «кошка» означает привести к именительному падежу все эти слова и получить «кошка»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81c35d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T20:23:43.831319Z",
     "iopub.status.busy": "2022-06-03T20:23:43.830949Z",
     "iopub.status.idle": "2022-06-03T20:23:43.838585Z",
     "shell.execute_reply": "2022-06-03T20:23:43.837454Z"
    },
    "papermill": {
     "duration": 0.014164,
     "end_time": "2022-06-03T20:23:43.840747",
     "exception": false,
     "start_time": "2022-06-03T20:23:43.826583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  На данном этапе мы создаем функцию которая позволит нам провести\n",
    "#  препроцессинг данных, а также токенизацию и лемматизацию данных.\n",
    "\n",
    "def preprocessing(x):\n",
    "    text = x\n",
    "    if text != None:\n",
    "        tock_dirt = word_tokenize(text, language=\"russian\")\n",
    "        morph_lst = []\n",
    "        tock = []\n",
    "        for word in tock_dirt:\n",
    "            word = re.sub(\"[^A-Za-zА-Яа-я]\", \" \", word)\n",
    "            for i in word.split():\n",
    "                if i != []:\n",
    "                    if i not in stopwords.words(\"russian\"):\n",
    "                        morph_lst.append(morph.parse(i)[0].normal_form)\n",
    "        return morph_lst\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75e1ab",
   "metadata": {},
   "source": [
    "## Работа с данными\n",
    "Выше мы описали каким образом подготавливались данные и представили для этого функцию. Загрузим данные и подготовим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30acc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T20:23:43.848481Z",
     "iopub.status.busy": "2022-06-03T20:23:43.848102Z",
     "iopub.status.idle": "2022-06-03T20:23:58.883725Z",
     "shell.execute_reply": "2022-06-03T20:23:58.882644Z"
    },
    "papermill": {
     "duration": 15.042873,
     "end_time": "2022-06-03T20:23:58.886461",
     "exception": false,
     "start_time": "2022-06-03T20:23:43.843588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  считаем данные\n",
    "\n",
    "data = pd.read_csv('../input/aihack-ved/opisanie_and_ved.csv', index_col=0)\n",
    "\n",
    "#  Применим к данным написанную ранее функцию\n",
    "\n",
    "data.OPISANIE = data.OPISANIE.apply(lambda x: preprocessing(x))\n",
    "data.OPISANIE = data.OPISANIE.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Сохраним данные для последующей обработки с помощью алгоритма/модели\n",
    "\n",
    "data.to_csv('data_preproc.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15743.883065,
   "end_time": "2022-06-04T00:45:42.712441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-03T20:23:18.829376",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
